<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }
    
    h1 {
        font-size:32px;
        font-weight:300;
    }
    
    .disclaimerbox {
        background-color: #eee;     
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    
    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    
    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    
    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }
    
    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }
    
    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
        5px 5px 0 0px #fff, /* The second layer */
        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
        10px 10px 0 0px #fff, /* The third layer */
        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
        15px 15px 0 0px #fff, /* The fourth layer */
        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
        20px 20px 0 0px #fff, /* The fifth layer */
        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
        25px 25px 0 0px #fff, /* The fifth layer */
        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
        5px 5px 0 0px #fff, /* The second layer */
        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
        10px 10px 0 0px #fff, /* The third layer */
        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }
    
    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }
    
    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
<head>
    <title>AV-Flow</title>
    <meta property="og:title" content="AV-Flow: Transforming Text to Audio-Visual Human-like Interactions"/>

    <!-- Get from Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src=""></script> 
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-75863369-6');
    </script>
</head>

<body>
    <br>
    <center>
        <span style="font-size:36px"><b>AV-Flow: Transforming Text to Audio-Visual Human-like Interactions</b></span><br></br>
        <table align=center>
            <table align=center width=1024px>
                <tr>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://aggelinacha.github.io/">Aggelina Chatziagapi</a><sup>1</sup></a></span>
                        </center>
                    </td>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a><sup>3</sup></a></span>
                        </center>
                    </td>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://hongyugong.github.io/">Hongyu Gong</a><sup>3</sup></a></span>
                        </center>
                    </td>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://zollhoefer.com/">Michael Zollh&oumlfer</a><sup>2</sup></a></span>
                        </center>
                    </td>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a><sup>1</sup></a></span>
                        </center>
                    </td>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://alexanderrichard.github.io/">Alexander Richard</a><sup>2</sup></a></span>
                        </center>
                    </td>
                </tr>
            </table><br>
            <table align=center>
                <tr>
                    <td align=center width=400px>
                        <center>
                            <span style="font-size:22px"><sup>1</sup>Stony Brook University</a></span>
                        </center>
                    </td>
                    <td align=center width=400px>
                        <center>
                            <span style="font-size:22px"><sup>2</sup>Codec Avatars Lab, Meta</a></span>
                        </center>
                    </td>
                    <td align=center width=400px>
                        <center>
                            <span style="font-size:22px"><sup>3</sup>Meta AI</a></span>
                        </center>
                    </td>
                </tr>
            </table><br>
            <!-- <table align=center>
                <tr>
                    <td align=center width=1024px>
                        <center>
                            <span style="font-size:24px"><i>arXiv 2024</i></a></span>
                        </center>
                    </td>
                </tr>
            </table><br></br> -->
            <table align=center width=256px>
                <tr>
                    <td align=center width=256px>
                        <center>
                            <span style="font-size:24px"><a href="https://arxiv.org/abs/2502.13133">ICCV 2025</a></span>
                        </center>
                    </td>
                    <!-- <td align=center width=120px>
                        <center>
                            <span style="font-size:24px">[GitHub]</a></span><br>
                        </center>
                    </td> -->
                </tr>
            </table><br></br>
        </table>
    </center>

    <center>
        <table align=center width=1024px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" style="width:1024px" src="teaser.png"/>
                    </center>
                <br></br>
                We present AV-Flow, a novel method for  <em>joint audio-visual generation</em> of 4D talking avatars, given text input only (e.g. obtained from an LLM). Inter-connected diffusion transformers ensure cross-modal communication, synthesizing synchronized speech, facial motion, and head motion, based on the flow matching objective. AV-Flow further enables empathetic dyadic interactions, by animating an <em>always-on</em> avatar that actively listens and reacts to the audio-visual input of a user.
                </td>
            </tr>
        </table>
    </center>

    <br>
    <hr>
    <br>

    <table align=center width=1024px>
        <center><h1>Abstract</h1></center>
        <tr>
            <td>
                We introduce AV-Flow, an audio-visual generative model that animates photo-realistic 4D talking avatars given only text input. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly. We demonstrate human-like speech synthesis, synchronized lip motion, lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations,
AV-Flow produces an always-on avatar, that actively listens and reacts to the audio-visual input of a user. 
Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars.
            </td>
        </tr>
    </table>
    <!-- <br> -->

<!--     <hr>
    <center><h1>Video</h1></center> -->
<!--     <p align="center">
        <iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
    </p>
 -->
<!--     <table align=center width=800px>
        <br>
        <tr>
            <center>
                <span style="font-size:28px"><a href=''>[Slides]</a>
                </span>
            </center>
        </tr>
    </table>
    <hr>

    <center><h1>Code</h1></center>

    <table align=center width=420px>
        <center>
            <tr>
                <td>
                </td>
            </tr>
        </center>
    </table>
    <table align=center width=400px>
        <tr>
            <td align=center width=400px>
                <center>
                    <td><img class="round" style="width:450px" src="./resources/method_diagram.png"/></td>
                </center>
            </td>
        </tr>
    </table>
    <table align=center width=850px>
        <center>
            <tr>
                <td>
                    Short description if wanted
                </td>
            </tr>
        </center>
    </table>
    <table align=center width=800px>
        <br>
        <tr><center>
            <span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
            </center>
        </span>
    </table> -->
    <br>
    <hr>
    <br>

    <table align=center width=1024px>
       <center><h1>Method</h1></center>
        <tr>
            <td><img style="width:1024px;" src="overview.png">
                <br></br>
                <b>Overview of AV-Flow. </b>Given any input text, our method synthesizes <em>expressive audio-visual 4D talking avatars</em>, jointly generating head and facial dynamics and the corresponding speech signal. Two parallel diffusion transformers with intermediate highway connections ensure communication between the audio and visual modalities. AV-Flow can be additionally conditioned on the audio-visual input of a user, in order to synthesize conversational avatars in dyadic interactions.
            </td>
        </tr>
    </table>

    <br>
    <hr>
    <br>
    <table align=center width=1024px>
        <center><h1>Demo</h1></center>
         <tr>
             <td align=center width=1024px>
                 <iframe style="width: 100%; min-height: 576px"
                     src="https://www.youtube.com/embed/eHS97hvYsF0">
                 </iframe>
             </td>
     </table>
    
    <br>
    <hr>
    <br>

    <table align=center width=1024px>
        <center><h1>BibTeX</h1></center>
         <tr>
            <td align=left width=1024px>
                If you find our work useful, please consider citing our paper:
            <!-- <td><span style="font-size:14pt"><center> -->
                <!-- <a href="./resources/bibtex.txt">[Bibtex]</a>  -->
            <div>
            <pre style="background-color: #f1f1f1; overflow-x: auto; max-width: 1024px;">
                <code>
        @article{chatziagapi2025avflow,
            title={AV-Flow: Transforming Text to Audio-Visual Human-like Interactions},
            author={Aggelina Chatziagapi and Louis-Philippe Morency and Hongyu Gong and Michael Zollh{\"o}fer and Dimitris Samaras and Alexander Richard},
            year={2025},
            journal={arXiv preprint arXiv:2502.13133},
        }
                </code>
            </pre>
        </div>
        </td>
            <!-- </center></td> -->
        </tr>
    </table>

    <!-- <br>
    <hr>
    <br> -->

    <!-- <table align=center width=1024>
        <tr>
            <td width=400px>
                <left>
                    <center><h1>Acknowledgements</h1></center>
                    This work is partly supported  by the Spanish government with the project MoHuCo PID2020-120049RB-I00 and Mar&iacutea de Maeztu Seal of Excellence MDM-2016-0656. This work was also supported by a gift from Adobe, Partner University Fund 4DVision Project, and the SUNY2020 Infrastructure Transportation Security Center.
                </left>
            </td>
        </tr>
    </table> -->

<br>
</body>
</html>

